#!/usr/bin/env python3
"""
åˆ†æTDDç”Ÿæˆçš„æµ‹è¯•è´¨é‡

è¿™ä¸ªè„šæœ¬ä½¿ç”¨å¥‘çº¦éªŒè¯å™¨æ¥åˆ†æTDDç®¡é“ç”Ÿæˆçš„æµ‹è¯•ä»£ç è´¨é‡ã€‚
"""

import json
import argparse
from pathlib import Path
from test_contract_validator import TestContractValidator, validate_multiple_tests, print_validation_summary


def load_tdd_results(result_file: str) -> list:
    """åŠ è½½TDDç»“æœæ–‡ä»¶"""
    samples = []
    with open(result_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                samples.append(json.loads(line))
    return samples


def extract_generated_tests(samples: list) -> list:
    """ä»æ ·æœ¬ä¸­æå–ç”Ÿæˆçš„æµ‹è¯•ä»£ç """
    test_codes = []
    valid_samples = []
    
    for sample in samples:
        if 'generated_tests' in sample and sample['generated_tests']:
            test_codes.append(sample['generated_tests'])
            valid_samples.append(sample)
        else:
            print(f"âš ï¸  Sample {sample.get('task_id', 'unknown')} missing generated_tests")
    
    print(f"ğŸ“Š Found {len(test_codes)} samples with generated tests out of {len(samples)} total")
    return test_codes, valid_samples


def analyze_tdd_test_quality(result_file: str, min_assertions: int = 3) -> None:
    """åˆ†æTDDæµ‹è¯•è´¨é‡"""
    print(f"ğŸ” Analyzing TDD test quality from: {result_file}")
    print("=" * 60)
    
    # åŠ è½½ç»“æœ
    samples = load_tdd_results(result_file)
    print(f"ğŸ“ Loaded {len(samples)} samples")
    
    # æå–ç”Ÿæˆçš„æµ‹è¯•
    test_codes, valid_samples = extract_generated_tests(samples)
    
    if not test_codes:
        print("âŒ No generated tests found to analyze")
        return
    
    # éªŒè¯æµ‹è¯•è´¨é‡
    print(f"\nğŸ” Validating {len(test_codes)} generated tests...")
    validation_results = validate_multiple_tests(test_codes, min_assertions)
    
    # æ‰“å°éªŒè¯æ‘˜è¦
    print_validation_summary(validation_results)
    
    # è¯¦ç»†åˆ†ææ¯ä¸ªæ ·æœ¬
    print(f"\nğŸ“‹ Detailed Analysis by Task:")
    print("-" * 60)
    
    for i, (sample, result) in enumerate(zip(valid_samples, validation_results)):
        task_id = sample.get('task_id', f'Task_{i}')
        print(f"\nTask {task_id}:")
        print(f"  âœ… Compliance: {result.is_compliant}")
        print(f"  ğŸ“Š Score: {result.score:.2f}")
        print(f"  ğŸ§ª Function Names: {'âœ…' if result.function_name_compliant else 'âŒ'}")
        print(f"  ğŸ“ Assertions: {'âœ…' if result.assertion_count_compliant else 'âŒ'}")
        print(f"  ğŸ¯ Boundary Tests: {'âœ…' if result.boundary_test_present else 'âŒ'}")
        print(f"  âš ï¸  Exception Tests: {'âœ…' if result.exception_test_present else 'âŒ'}")
        print(f"  ğŸ Pytest Format: {'âœ…' if result.pytest_format_compliant else 'âŒ'}")
        
        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        if result.details:
            if 'function_names' in result.details:
                print(f"  ğŸ“› Functions: {', '.join(result.details['function_names'])}")
            if 'assertion_count' in result.details:
                print(f"  ğŸ“Š Assertions: {result.details['assertion_count']}")
            if 'boundary_test_details' in result.details:
                boundary_info = result.details['boundary_test_details']
                if boundary_info['has_boundary_tests']:
                    print(f"  ğŸ¯ Boundary Types: {', '.join(boundary_info['boundary_types'])}")
            if 'exception_test_details' in result.details:
                exception_info = result.details['exception_test_details']
                if exception_info['has_exception_tests']:
                    print(f"  âš ï¸  Exception Types: {', '.join(exception_info['exception_types'])}")
    
    # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
    generate_quality_report(valid_samples, validation_results, result_file)


def generate_quality_report(samples: list, results: list, result_file: str) -> None:
    """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
    print(f"\nğŸ“Š GENERATING QUALITY REPORT")
    print("=" * 60)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_tests = len(results)
    compliant_tests = sum(1 for r in results if r.is_compliant)
    avg_score = sum(r.score for r in results) / total_tests if results else 0
    
    # å„é¡¹æŒ‡æ ‡çš„ç»Ÿè®¡
    metrics = {
        'Function Names': 'function_name_compliant',
        'Assertion Count': 'assertion_count_compliant', 
        'Boundary Tests': 'boundary_test_present',
        'Exception Tests': 'exception_test_present',
        'Pytest Format': 'pytest_format_compliant'
    }
    
    metric_stats = {}
    for metric_name, attr_name in metrics.items():
        compliant_count = sum(1 for r in results if getattr(r, attr_name))
        percentage = compliant_count / total_tests * 100 if total_tests > 0 else 0
        metric_stats[metric_name] = {
            'compliant': compliant_count,
            'total': total_tests,
            'percentage': percentage
        }
    
    # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶å
    report_file = result_file.replace('.jsonl', '_quality_report.txt')
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("TDD TEST QUALITY ANALYSIS REPORT\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"Source File: {result_file}\n")
        f.write(f"Analysis Date: {__import__('datetime').datetime.now().isoformat()}\n\n")
        
        f.write("SUMMARY STATISTICS\n")
        f.write("-" * 30 + "\n")
        f.write(f"Total Tests Analyzed: {total_tests}\n")
        f.write(f"Compliant Tests: {compliant_tests} ({compliant_tests/total_tests*100:.1f}%)\n")
        f.write(f"Average Quality Score: {avg_score:.2f}\n\n")
        
        f.write("COMPLIANCE BY CATEGORY\n")
        f.write("-" * 30 + "\n")
        for metric_name, stats in metric_stats.items():
            f.write(f"{metric_name:20} {stats['compliant']:3d}/{stats['total']} ({stats['percentage']:5.1f}%)\n")
        
        f.write("\nDETAILED ANALYSIS\n")
        f.write("-" * 30 + "\n")
        
        for i, (sample, result) in enumerate(zip(samples, results)):
            task_id = sample.get('task_id', f'Task_{i}')
            f.write(f"\nTask {task_id}:\n")
            f.write(f"  Compliance: {result.is_compliant}\n")
            f.write(f"  Score: {result.score:.2f}\n")
            f.write(f"  Function Names: {'âœ…' if result.function_name_compliant else 'âŒ'}\n")
            f.write(f"  Assertions: {'âœ…' if result.assertion_count_compliant else 'âŒ'}\n")
            f.write(f"  Boundary Tests: {'âœ…' if result.boundary_test_present else 'âŒ'}\n")
            f.write(f"  Exception Tests: {'âœ…' if result.exception_test_present else 'âŒ'}\n")
            f.write(f"  Pytest Format: {'âœ…' if result.pytest_format_compliant else 'âŒ'}\n")
            
            if result.details:
                if 'function_names' in result.details:
                    f.write(f"  Functions: {', '.join(result.details['function_names'])}\n")
                if 'assertion_count' in result.details:
                    f.write(f"  Assertions: {result.details['assertion_count']}\n")
    
    print(f"ğŸ“„ Quality report saved to: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Analyze TDD test quality")
    parser.add_argument("result_file", help="TDD result file to analyze")
    parser.add_argument("--min_assertions", type=int, default=3, 
                       help="Minimum number of assertions required (default: 3)")
    
    args = parser.parse_args()
    
    if not Path(args.result_file).exists():
        print(f"âŒ Error: File {args.result_file} not found")
        return
    
    try:
        analyze_tdd_test_quality(args.result_file, args.min_assertions)
    except Exception as e:
        print(f"âŒ Error during analysis: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
