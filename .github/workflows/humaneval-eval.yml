name: HumanEval Evaluation

on:
  push:
    tags:
      - 'v*'
    paths:
      - 'benchmarks/humaneval/data/langchain_batch_164_tasks*.jsonl'
      - '.github/workflows/humaneval-eval.yml'
  workflow_dispatch:

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install HumanEval
        run: |
          pip install -e benchmarks/humaneval/

      - name: Evaluate all 164-task results
        run: |
          for f in benchmarks/humaneval/data/langchain_batch_164_tasks*.jsonl; do
            echo "Evaluating $f"
            evaluate_functional_correctness "$f" --problem_file=benchmarks/humaneval/data/HumanEval.jsonl
          done

      - name: Upload evaluation results
        uses: actions/upload-artifact@v4
        with:
          name: humaneval-results
          path: benchmarks/humaneval/data/langchain_batch_164_tasks*_results.jsonl 