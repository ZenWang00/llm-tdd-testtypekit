#!/usr/bin/env python3
"""
MBPPå‚æ•°åŒ–å®éªŒè„šæœ¬

è¿è¡Œä¸€æ¬¡å®Œæ•´çš„TDD (200 tasks, T:0.0) å’Œå¤šæ¬¡Test Only (200 tasks, ä¸åŒtemperature)
ç„¶åè¯„ä¼°æµ‹è¯•ç”¨ä¾‹è´¨é‡æŒ‡æ ‡
"""

import os
import sys
import json
import datetime
from typing import Dict, Any, List
from tqdm import tqdm
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from pipelines.batch_mbpp_tdd import TDDMBPPBatchPipeline
from pipelines.batch_mbpp_test_only import TestOnlyMBPPBatchPipeline
from pipelines.lc_chain.generator import generate_tests_for_mbpp


def run_full_tdd_mbpp(num_tasks: int = 200, start_task: int = 0, model: str = "gpt-4o-mini", temp_dir: str = None):
    """è¿è¡Œå®Œæ•´çš„TDD pipeline (200 tasks, T:0.0)"""
    print(f"\n{'='*60}")
    print(f"Running Full TDD MBPP Experiment")
    print(f"Tasks: {num_tasks}, Start: {start_task}, Model: {model}")
    print(f"{'='*60}")
    
    try:
        pipeline = TDDMBPPBatchPipeline(model)
        
        # ä¿®æ”¹è¾“å‡ºè·¯å¾„åˆ°ä¸´æ—¶ç›®å½•
        if temp_dir:
            # é‡å†™ pipeline çš„ get_output_filename æ–¹æ³•
            def temp_output_filename(num_tasks, prefix):
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"{prefix}_{num_tasks}_tasks_{model}_{timestamp}.jsonl"
                return os.path.join(temp_dir, filename)
            
            pipeline.get_output_filename = temp_output_filename
        
        output_file = pipeline.run_batch_pipeline(
            problem_file="benchmarks/mbpp/data/langchain_mbpp_batch_974_tasks_gpt-4o-mini_20250830_220247.jsonl",
            num_tasks=num_tasks,
            start_task=start_task
        )
        
        # è¯»å–ç»“æœç”¨äºè¿”å›
        results = []
        if output_file and os.path.exists(output_file):
            with open(output_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        results.append(json.loads(line))
        
        print(f"\nâœ… Full TDD completed successfully!")
        print(f"Output file: {output_file}")
        print(f"Generated {len(results)} samples")
        
        return output_file, results
        
    except Exception as e:
        print(f"âŒ Full TDD failed: {e}")
        return None, []


def run_test_only_mbpp(num_tasks: int, start_task: int, temperature: float, 
                       model: str = "gpt-4o-mini", temp_dir: str = None):
    """è¿è¡ŒTest Only pipeline (æŒ‡å®štemperature)"""
    print(f"\n{'='*60}")
    print(f"Running Test Only MBPP Experiment")
    print(f"Tasks: {num_tasks}, Start: {start_task}, Temperature: {temperature}, Model: {model}")
    print(f"{'='*60}")
    
    try:
        # åˆ›å»ºè‡ªå®šä¹‰çš„Test Only pipeline
        pipeline = TestOnlyMBPPBatchPipeline(model)
        
        # è¯»å–é—®é¢˜
        problems = pipeline.read_problems("benchmarks/mbpp/data/langchain_mbpp_batch_974_tasks_gpt-4o-mini_20250830_220247.jsonl")
        
        # é€‰æ‹©ä»»åŠ¡
        task_ids = list(problems.keys())
        task_ids.sort(key=int)
        end_task = min(start_task + num_tasks, len(task_ids))
        selected_task_ids = task_ids[start_task:end_task]
        
        print(f"Processing {len(selected_task_ids)} tasks...")
        
        results = []
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡
        for task_id in tqdm(selected_task_ids, desc=f"Test Generation (T:{temperature})"):
            problem = problems[task_id]
            try:
                description = problem['prompt']  # ä¿®å¤ï¼šä½¿ç”¨'prompt'è€Œä¸æ˜¯'text'
                reference_code = problem.get('reference_code', '')
                
                # ç”Ÿæˆæµ‹è¯•ï¼ˆä½¿ç”¨æŒ‡å®štemperatureï¼‰
                generated_tests = generate_tests_for_mbpp(
                    description, reference_code, model, temperature=temperature
                )
                
                # åˆ›å»ºæ ·æœ¬
                sample = {
                    "task_id": task_id,
                    "prompt": problem['prompt'],
                    "reference_code": reference_code,
                    "generated_tests": generated_tests,
                    "method": "test_only_mbpp",
                    "model": model,
                    "timestamp": datetime.datetime.now().isoformat(),
                    "stage": "test_generation_only",
                    "temperature": temperature
                }
                results.append(sample)
                
            except Exception as e:
                print(f"Error in task {task_id}: {e}")
                sample = {
                    "task_id": task_id,
                    "prompt": problem['prompt'],
                    "reference_code": reference_code,
                    "generated_tests": "",
                    "method": "test_only_mbpp",
                    "model": model,
                    "timestamp": datetime.datetime.now().isoformat(),
                    "stage": "test_generation_only",
                    "temperature": temperature,
                    "error": str(e)
                }
                results.append(sample)
        
        # ä¿å­˜ç»“æœåˆ°ä¸´æ—¶ç›®å½•
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        if temp_dir:
            output_file = os.path.join(temp_dir, f"test_only_mbpp_batch_{num_tasks}_tasks_T{temperature}_{model}_{timestamp}.jsonl")
        else:
            output_file = f"benchmarks/mbpp/data/test_only_mbpp_batch_{num_tasks}_tasks_T{temperature}_{model}_{timestamp}.jsonl"
        
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            for sample in results:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        
        print(f"\nâœ… Test Only (T:{temperature}) completed successfully!")
        print(f"Output file: {output_file}")
        print(f"Generated {len(results)} samples")
        
        return output_file, results
        
    except Exception as e:
        print(f"âŒ Test Only (T:{temperature}) failed: {e}")
        return None, []


def analyze_test_quality(output_files: List[str]):
    """åˆ†ææ‰€æœ‰è¾“å‡ºæ–‡ä»¶çš„æµ‹è¯•è´¨é‡"""
    print(f"\n{'='*60}")
    print(f"Analyzing Test Quality for All Experiments")
    print(f"{'='*60}")
    
    for output_file in output_files:
        if not output_file or not os.path.exists(output_file):
            continue
            
        print(f"\n--- Analyzing: {os.path.basename(output_file)} ---")
        
        try:
            # è¯»å–ç»“æœ
            samples = []
            with open(output_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        samples.append(json.loads(line))
            
            if not samples:
                print("No samples found")
                continue
            
            # æå–å‚æ•°ä¿¡æ¯
            temperature = samples[0].get('temperature', 'N/A')
            method = samples[0].get('method', 'N/A')
            num_samples = len(samples)
            
            print(f"Method: {method}")
            print(f"Temperature: {temperature}")
            print(f"Total samples: {num_samples}")
            
            # ç»Ÿè®¡é”™è¯¯
            errors = [s for s in samples if 'error' in s]
            if errors:
                print(f"Errors: {len(errors)}")
            
            # ç»Ÿè®¡æˆåŠŸç”Ÿæˆçš„æµ‹è¯•
            successful = [s for s in samples if s.get('generated_tests') and 'error' not in s]
            print(f"Successful generations: {len(successful)}")
            
            # è®¡ç®—å¹³å‡æµ‹è¯•é•¿åº¦
            if successful:
                avg_length = sum(len(s['generated_tests']) for s in successful) / len(successful)
                print(f"Average test length: {avg_length:.0f} characters")
            
        except Exception as e:
            print(f"Error analyzing {output_file}: {e}")


def run_parameter_experiment(num_tasks: int = 200, start_task: int = 0, 
                           model: str = "gpt-4o-mini"):
    """è¿è¡Œå®Œæ•´çš„å‚æ•°åŒ–å®éªŒ"""
    print(f"ğŸš€ Starting MBPP Parameter Experiment")
    print(f"Tasks: {num_tasks}, Start: {start_task}, Model: {model}")
    print(f"Time: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    temp_dir = f"benchmarks/mbpp/data/temp_{timestamp}"
    os.makedirs(temp_dir, exist_ok=True)
    print(f"ğŸ“ Created temporary directory: {temp_dir}")
    
    output_files = []
    
    # 1. è¿è¡Œå®Œæ•´TDD (baseline)
    print(f"\nğŸ“Š Step 1: Running Full TDD (Baseline)")
    tdd_file, tdd_results = run_full_tdd_mbpp(num_tasks, start_task, model, temp_dir)
    if tdd_file:
        output_files.append(tdd_file)
    
    # 2. è¿è¡Œå¤šæ¬¡Test Only (ä¸åŒtemperature)
    temperatures = [0.1, 0.3, 0.5, 0.7, 0.9]
    
    print(f"\nğŸ“Š Step 2: Running Test Only with Different Temperatures")
    for i, temp in enumerate(temperatures, 1):
        print(f"\n--- Experiment {i}/{len(temperatures)} ---")
        test_file, test_results = run_test_only_mbpp(num_tasks, start_task, temp, model, temp_dir)
        if test_file:
            output_files.append(test_file)
    
    # 3. åˆ†ææ‰€æœ‰ç»“æœ
    print(f"\nğŸ“Š Step 3: Analyzing All Results")
    analyze_test_quality(output_files)
    
    # 4. è¿è¡Œæµ‹è¯•è´¨é‡åˆ†æ
    print(f"\nğŸ“Š Step 4: Running Test Quality Analysis")
    print(f"Analyzing data in: {temp_dir}")
    
    # ä¸ºæ¯ä¸ªè¾“å‡ºæ–‡ä»¶è¿è¡Œåˆ†æ
    for output_file in output_files:
        if output_file and os.path.exists(output_file):
            print(f"Analyzing: {os.path.basename(output_file)}")
            try:
                # è¿è¡Œåˆ†æè„šæœ¬
                import subprocess
                result = subprocess.run([
                    "python", "analyze_tdd_comprehensive.py", 
                    output_file
                ], capture_output=True, text=True)
                
                if result.returncode == 0:
                    print(f"âœ… Analysis completed for {os.path.basename(output_file)}")
                else:
                    print(f"âš ï¸ Analysis failed for {os.path.basename(output_file)}: {result.stderr}")
            except Exception as e:
                print(f"âš ï¸ Could not run analysis for {os.path.basename(output_file)}: {e}")
    
    # 5. ç”Ÿæˆå®éªŒæŠ¥å‘Š
    print(f"\nğŸ“Š Step 5: Generating Experiment Report")
    print(f"Analyzing data in: {temp_dir}")
    
    try:
        # è°ƒç”¨æŠ¥å‘Šç”Ÿæˆè„šæœ¬
        import subprocess
        result = subprocess.run([
            "python", "generate_experiment_report.py", 
            "--data_dir", temp_dir,
            "--output", f"report/mbpp_experiment_{timestamp}.md"
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            print(f"âœ… Report generated successfully!")
            print(f"Report file: report/mbpp_experiment_{timestamp}.md")
        else:
            print(f"âš ï¸ Report generation failed: {result.stderr}")
    except Exception as e:
        print(f"âš ï¸ Could not generate report automatically: {e}")
        print("You can manually run:")
        print(f"python generate_experiment_report.py --data_dir {temp_dir}")
    
    print(f"\nğŸ‰ Parameter experiment completed!")
    print(f"Generated {len(output_files)} output files in: {temp_dir}")
    for file in output_files:
        print(f"  - {os.path.basename(file)}")
    
    return output_files, temp_dir


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Run MBPP parameter experiment")
    parser.add_argument("--num_tasks", type=int, default=200, help="Number of tasks to process")
    parser.add_argument("--start_task", type=int, default=0, help="Starting task index")
    parser.add_argument("--model", default="gpt-4o-mini", help="Model to use")
    
    args = parser.parse_args()
    
    try:
        output_files, temp_dir = run_parameter_experiment(
            num_tasks=args.num_tasks,
            start_task=args.start_task,
            model=args.model
        )
        
        print(f"\nâœ… Experiment completed successfully!")
        print(f"Total output files: {len(output_files)}")
        print(f"Data directory: {temp_dir}")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ Experiment interrupted by user")
        return 1
    except Exception as e:
        print(f"\nâŒ Experiment failed: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
