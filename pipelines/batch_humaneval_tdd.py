#!/usr/bin/env python3
"""
TDD HumanEval Pipeline
Two-stage pipeline: test generation -> implementation generation
"""

import os
import sys
import datetime
sys.path.append('.')

from human_eval.data import write_jsonl, read_problems
from pipelines.batch_base import BaseBatchPipeline
from pipelines.lc_chain.generator import (
    generate_tests_for_humaneval, 
    generate_implementation_with_tests_humaneval
)

class TDDHumanEvalBatchPipeline(BaseBatchPipeline):
    """TDDç‰ˆæœ¬çš„HumanEvalæ‰¹å¤„ç†ç®¡é“"""
    
    def read_problems(self, problem_file):
        """å¤ç”¨HumanEvalçš„é—®é¢˜è¯»å–é€»è¾‘"""
        return read_problems(problem_file)
    
    def get_output_filename(self, num_tasks, prefix):
        """ç”ŸæˆTDDç‰ˆæœ¬çš„è¾“å‡ºæ–‡ä»¶å"""
        return f"benchmarks/humaneval/data/{prefix}_{num_tasks}_tasks_{self.model}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
    
    def create_sample(self, task_id, problem, completion, error=None, generated_tests=None):
        """åˆ›å»ºTDDç‰ˆæœ¬çš„æ ·æœ¬"""
        sample = {
            "task_id": task_id,
            "prompt": problem['prompt'],
            "completion": completion,
            "method": "tdd_humaneval",  # æ ‡è¯†è¿™æ˜¯TDDæ–¹æ³•
            "model": self.model,
            "timestamp": datetime.datetime.now().isoformat(),
            "entry_point": problem.get('entry_point', ''),
            "canonical_solution": problem.get('canonical_solution', ''),
            "test": problem.get('test', ''),
            "generated_tests": generated_tests,  # æ–°å¢ï¼šç”Ÿæˆçš„æµ‹è¯•
            "tdd_stage": "implementation"  # æ ‡è¯†è¿™æ˜¯å®ç°é˜¶æ®µçš„ç»“æœ
        }
        if error:
            sample["error"] = error
        return sample
    
    def save_results(self, output_file, samples):
        """å¤ç”¨HumanEvalçš„ä¿å­˜é€»è¾‘"""
        write_jsonl(output_file, samples)
    
    def get_prompt(self, problem):
        """TDDæµç¨‹ä¸éœ€è¦ä¼ ç»Ÿpromptï¼Œè¿”å›å‡½æ•°ç­¾å"""
        return problem['prompt']
    
    def run_batch_pipeline(self, problem_file, num_tasks=10, start_task=0):
        """é‡å†™æ‰¹å¤„ç†é€»è¾‘ï¼Œå®ç°ä¸¤é˜¶æ®µTDDæµç¨‹"""
        try:
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            output_file = self.generate_batch_output_filename(num_tasks, "tdd_humaneval_batch")
            
            # è¯»å–é—®é¢˜
            print(f"Reading HumanEval problem file: {problem_file}")
            problems = self.read_problems(problem_file)
            print(f"Total HumanEval problems available: {len(problems)}")
            
            # é€‰æ‹©ä»»åŠ¡
            task_items = list(problems.items())
            end_task = min(start_task + num_tasks, len(task_items))
            selected_tasks = task_items[start_task:end_task]
            
            print(f"Processing TDD HumanEval tasks {start_task} to {end_task-1} ({len(selected_tasks)} tasks)")
            print(f"Using model: {self.model}")
            print("=" * 50)
            
            # ä¸¤é˜¶æ®µå¤„ç†
            samples = []
            for task_id, problem in selected_tasks:
                try:
                    prompt = problem['prompt']
                    canonical_solution = problem.get('canonical_solution', '')
                    
                    # é˜¶æ®µ1ï¼šç”Ÿæˆæµ‹è¯•
                    print(f"\nğŸ”„ Stage 1: Generating tests for task {task_id}")
                    generated_tests = generate_tests_for_humaneval(prompt, canonical_solution, self.model)
                    print(f"âœ… Tests generated: {len(generated_tests)} characters")
                    
                    # é˜¶æ®µ2ï¼šæ ¹æ®æµ‹è¯•ç”Ÿæˆå®ç°
                    print(f"ğŸ”„ Stage 2: Generating implementation for task {task_id}")
                    implementation = generate_implementation_with_tests_humaneval(
                        prompt, generated_tests, self.model
                    )
                    print(f"âœ… Implementation generated: {len(implementation)} characters")
                    
                    # åˆ›å»ºæ ·æœ¬
                    sample = self.create_sample(task_id, problem, implementation, generated_tests=generated_tests)
                    samples.append(sample)
                    
                except Exception as e:
                    print(f"Error processing {task_id}: {e}")
                    sample = self.create_sample(task_id, problem, "    pass", error=str(e))
                    samples.append(sample)
            
            # ä¿å­˜ç»“æœ
            self.save_results(output_file, samples)
               
            print(f"\nResults saved: {output_file}")
            
            # æ‰“å°æ‘˜è¦
            successful = sum(1 for s in samples if 'error' not in s)
            failed = len(samples) - successful
            print(f"Summary: {successful} successful, {failed} failed")
            
            return output_file
            
        except Exception as e:
            print(f"TDD batch processing failed: {e}")
            return None

def run_tdd_humaneval_pipeline(problem_file="benchmarks/humaneval/data/HumanEval.jsonl", 
                              num_tasks=10, 
                              model="gpt-4o-mini",
                              start_task=0):
    """è¿è¡ŒTDD HumanEvalç®¡é“"""
    pipeline = TDDHumanEvalBatchPipeline(model)
    return pipeline.run_batch_pipeline(problem_file, num_tasks, start_task)

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="TDD HumanEval Pipeline")
    parser.add_argument("--num_tasks", type=int, default=10, help="Number of tasks to process")
    parser.add_argument("--model", type=str, default="gpt-4o-mini", help="Model to use")
    parser.add_argument("--start_task", type=int, default=0, help="Starting task index")
    parser.add_argument("--problem_file", type=str, default="benchmarks/humaneval/data/HumanEval.jsonl", 
                       help="HumanEval problem file path")
    
    args = parser.parse_args()
    
    print("Starting TDD HumanEval Pipeline")
    print("=" * 50)
    
    output_file = run_tdd_humaneval_pipeline(
        problem_file=args.problem_file,
        num_tasks=args.num_tasks,
        model=args.model,
        start_task=args.start_task
    )
    
    if output_file:
        print("\nTDD pipeline completed successfully!")
        print(f"Result file: {output_file}")
        print("\nNext steps:")
        print(f"1. Run evaluation: evaluate_functional_correctness {output_file} --problem_file={args.problem_file}")
        print(f"2. View results: cat {output_file}")
    else:
        print("\nTDD pipeline failed")
        sys.exit(1)

if __name__ == "__main__":
    main()
