{"task_id": "HumanEval/0", "prompt": "from typing import List\n\n\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n    given threshold.\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n    True\n    \"\"\"\n", "completion": "if threshold < 0:\n        raise ValueError(\"Threshold must be non-negative\")\n    if len(numbers) < 2:\n        return False\n    numbers.sort()\n    for i in range(len(numbers) - 1):\n        if numbers[i + 1] - numbers[i] < threshold:\n            return True\n    return False", "method": "tdd_humaneval", "model": "gpt-4o-mini", "timestamp": "2025-08-30T14:06:38.787821", "entry_point": "has_close_elements", "canonical_solution": "    for idx, elem in enumerate(numbers):\n        for idx2, elem2 in enumerate(numbers):\n            if idx != idx2:\n                distance = abs(elem - elem2)\n                if distance < threshold:\n                    return True\n\n    return False\n", "test": "\n\nMETADATA = {\n    'author': 'jt',\n    'dataset': 'test'\n}\n\n\ndef check(candidate):\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\n\n", "generated_tests": "import pytest\n\ndef test_no_close_elements():\n    assert not has_close_elements([1.0, 2.0, 3.0], 0.5)\n\ndef test_close_elements_found():\n    assert has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n\ndef test_identical_elements():\n    assert has_close_elements([1.0, 1.0, 2.0], 0.1)\n\ndef test_negative_numbers():\n    assert has_close_elements([-1.0, -1.4, -2.0], 0.5)\n\ndef test_mixed_numbers():\n    assert has_close_elements([-1.0, 0.0, 1.0], 0.1)\n\ndef test_empty_list():\n    assert not has_close_elements([], 0.5)\n\ndef test_single_element():\n    assert not has_close_elements([1.0], 0.5)\n\ndef test_large_threshold():\n    assert has_close_elements([1.0, 2.0, 3.0], 5.0)\n\ndef test_small_threshold_with_large_numbers():\n    assert not has_close_elements([1000.0, 1001.0, 1002.0], 0.1)\n\ndef test_float_precision():\n    assert has_close_elements([0.1, 0.1000000001], 0.0000001)\n\ndef test_threshold_zero():\n    assert not has_close_elements([1.0, 2.0], 0.0)\n\ndef test_negative_threshold():\n    with pytest.raises(ValueError):\n        has_close_elements([1.0, 2.0], -0.1)", "tdd_stage": "implementation"}
{"task_id": "HumanEval/1", "prompt": "from typing import List\n\n\ndef separate_paren_groups(paren_string: str) -> List[str]:\n    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n    separate those group into separate strings and return the list of those.\n    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n    Ignore any spaces in the input string.\n    >>> separate_paren_groups('( ) (( )) (( )( ))')\n    ['()', '(())', '(()())']\n    \"\"\"\n", "completion": "paren_string = paren_string.replace(\" \", \"\")\n    result = []\n    balance = 0\n    current_group = []\n\n    for char in paren_string:\n        if char == '(':\n            if balance == 0 and current_group:\n                result.append(''.join(current_group))\n                current_group = []\n            current_group.append(char)\n            balance += 1\n        elif char == ')':\n            if balance == 0:\n                raise IndexError(\"Unbalanced parentheses\")\n            current_group.append(char)\n            balance -= 1\n            if balance == 0:\n                result.append(''.join(current_group))\n                current_group = []\n\n    if balance != 0:\n        raise IndexError(\"Unbalanced parentheses\")\n\n    return result", "method": "tdd_humaneval", "model": "gpt-4o-mini", "timestamp": "2025-08-30T14:06:50.358764", "entry_point": "separate_paren_groups", "canonical_solution": "    result = []\n    current_string = []\n    current_depth = 0\n\n    for c in paren_string:\n        if c == '(':\n            current_depth += 1\n            current_string.append(c)\n        elif c == ')':\n            current_depth -= 1\n            current_string.append(c)\n\n            if current_depth == 0:\n                result.append(''.join(current_string))\n                current_string.clear()\n\n    return result\n", "test": "\n\nMETADATA = {\n    'author': 'jt',\n    'dataset': 'test'\n}\n\n\ndef check(candidate):\n    assert candidate('(()()) ((())) () ((())()())') == [\n        '(()())', '((()))', '()', '((())()())'\n    ]\n    assert candidate('() (()) ((())) (((())))') == [\n        '()', '(())', '((()))', '(((())))'\n    ]\n    assert candidate('(()(())((())))') == [\n        '(()(())((())))'\n    ]\n    assert candidate('( ) (( )) (( )( ))') == ['()', '(())', '(()())']\n", "generated_tests": "import pytest\n\ndef test_basic_functionality():\n    assert separate_paren_groups('( ) (( )) (( )( ))') == ['()', '(())', '(()())']\n    assert separate_paren_groups('()') == ['()']\n    assert separate_paren_groups('((()))') == ['((()))']\n    assert separate_paren_groups('()()()') == ['()', '()', '()']\n    assert separate_paren_groups('(()())(())') == ['(()())', '(())']\n\ndef test_edge_cases():\n    assert separate_paren_groups('') == []\n    assert separate_paren_groups(' ') == []\n    assert separate_paren_groups('() ') == ['()']\n    assert separate_paren_groups(' ( ) ') == ['()']\n    assert separate_paren_groups('(( ) ( ))') == ['()', '()']\n    assert separate_paren_groups('()()()()()') == ['()', '()', '()', '()']\n\ndef test_invalid_inputs():\n    with pytest.raises(IndexError):\n        separate_paren_groups('(')\n    with pytest.raises(IndexError):\n        separate_paren_groups(')')\n    with pytest.raises(IndexError):\n        separate_paren_groups('(()')\n    with pytest.raises(IndexError):\n        separate_paren_groups('())')\n    assert separate_paren_groups('((())') == []\n    assert separate_paren_groups('())(') == []\n    assert separate_paren_groups(')(') == []\n\ndef test_complex_cases():\n    assert separate_paren_groups('(((()))) (( )) (())') == ['(((())))', '()', '(())']\n    assert separate_paren_groups('(( ) ( ) ( ( ) ))') == ['()', '()', '(()())']\n    assert separate_paren_groups('()()(()())(())') == ['()', '()', '(()())', '(())']", "tdd_stage": "implementation"}
{"task_id": "HumanEval/2", "prompt": "\n\ndef truncate_number(number: float) -> float:\n    \"\"\" Given a positive floating point number, it can be decomposed into\n    and integer part (largest integer smaller than given number) and decimals\n    (leftover part always smaller than 1).\n\n    Return the decimal part of the number.\n    >>> truncate_number(3.5)\n    0.5\n    \"\"\"\n", "completion": "if not isinstance(number, (float, int)) or number < 0:\n        raise TypeError(\"Input must be a positive floating point number.\")\n    return number - int(number)", "method": "tdd_humaneval", "model": "gpt-4o-mini", "timestamp": "2025-08-30T14:06:58.626022", "entry_point": "truncate_number", "canonical_solution": "    return number % 1.0\n", "test": "\n\nMETADATA = {\n    'author': 'jt',\n    'dataset': 'test'\n}\n\n\ndef check(candidate):\n    assert candidate(3.5) == 0.5\n    assert abs(candidate(1.33) - 0.33) < 1e-6\n    assert abs(candidate(123.456) - 0.456) < 1e-6\n", "generated_tests": "import pytest\n\ndef test_truncate_number_basic_functionality():\n    assert truncate_number(3.5) == 0.5\n    assert truncate_number(2.75) == 0.75\n    assert truncate_number(0.99) == 0.99\n    assert truncate_number(5.0) == 0.0\n    assert truncate_number(10.123) == 0.123\n\ndef test_truncate_number_edge_cases():\n    assert truncate_number(0.0) == 0.0\n    assert truncate_number(1.0) == 0.0\n    assert truncate_number(1.999) == 0.999\n    assert truncate_number(100.001) == 0.001\n\ndef test_truncate_number_large_numbers():\n    assert truncate_number(1e6 + 0.123) == 0.123\n    assert truncate_number(1e12 + 0.999) == 0.999\n\ndef test_truncate_number_negative_cases():\n    with pytest.raises(TypeError):\n        truncate_number(\"string\")\n    with pytest.raises(TypeError):\n        truncate_number(None)\n    with pytest.raises(TypeError):\n        truncate_number([])\n\ndef test_truncate_number_boundary_conditions():\n    assert truncate_number(1e-10) == 1e-10\n    assert truncate_number(1e-10 + 1) == 1e-10", "tdd_stage": "implementation"}
{"task_id": "HumanEval/3", "prompt": "from typing import List\n\n\ndef below_zero(operations: List[int]) -> bool:\n    \"\"\" You're given a list of deposit and withdrawal operations on a bank account that starts with\n    zero balance. Your task is to detect if at any point the balance of account fallls below zero, and\n    at that point function should return True. Otherwise it should return False.\n    >>> below_zero([1, 2, 3])\n    False\n    >>> below_zero([1, 2, -4, 5])\n    True\n    \"\"\"\n", "completion": "balance = 0\n    for operation in operations:\n        balance += operation\n        if balance < 0:\n            return True\n    return False", "method": "tdd_humaneval", "model": "gpt-4o-mini", "timestamp": "2025-08-30T14:07:06.998406", "entry_point": "below_zero", "canonical_solution": "    balance = 0\n\n    for op in operations:\n        balance += op\n        if balance < 0:\n            return True\n\n    return False\n", "test": "\n\nMETADATA = {\n    'author': 'jt',\n    'dataset': 'test'\n}\n\n\ndef check(candidate):\n    assert candidate([]) == False\n    assert candidate([1, 2, -3, 1, 2, -3]) == False\n    assert candidate([1, 2, -4, 5, 6]) == True\n    assert candidate([1, -1, 2, -2, 5, -5, 4, -4]) == False\n    assert candidate([1, -1, 2, -2, 5, -5, 4, -5]) == True\n    assert candidate([1, -2, 2, -2, 5, -5, 4, -4]) == True\n", "generated_tests": "import pytest\n\ndef test_no_operations():\n    assert below_zero([]) == False\n\ndef test_single_deposit():\n    assert below_zero([5]) == False\n\ndef test_single_withdrawal_not_below_zero():\n    assert below_zero([-1]) == False\n\ndef test_single_withdrawal_below_zero():\n    assert below_zero([-1, 1]) == False\n    assert below_zero([-1, -1]) == True\n\ndef test_multiple_operations_no_below_zero():\n    assert below_zero([10, 5, 3]) == False\n    assert below_zero([5, 5, -3, 2]) == False\n\ndef test_multiple_operations_with_below_zero():\n    assert below_zero([10, -5, -6]) == True\n    assert below_zero([1, 2, -4, 5]) == True\n    assert below_zero([-1, 2, -2]) == True\n\ndef test_operations_returning_to_zero():\n    assert below_zero([1, -1]) == False\n    assert below_zero([2, -2, 3, -3]) == False\n\ndef test_large_numbers():\n    assert below_zero([1000000, -1000001]) == True\n    assert below_zero([1000000, -999999]) == False\n\ndef test_edge_case_with_zero():\n    assert below_zero([0, 0, 0]) == False\n    assert below_zero([0, -1]) == True\n\ndef test_negative_operations_only():\n    assert below_zero([-1, -1, -1]) == True\n    assert below_zero([-1, 0, -1]) == True\n\ndef test_mixed_operations():\n    assert below_zero([5, -3, -3]) == True\n    assert below_zero([-2, 1, 1]) == False", "tdd_stage": "implementation"}
{"task_id": "HumanEval/4", "prompt": "from typing import List\n\n\ndef mean_absolute_deviation(numbers: List[float]) -> float:\n    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\n    around the mean of this dataset.\n    Mean Absolute Deviation is the average absolute difference between each\n    element and a centerpoint (mean in this case):\n    MAD = average | x - x_mean |\n    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\n    1.0\n    \"\"\"\n", "completion": "if not isinstance(numbers, list) or any(not isinstance(x, (int, float)) for x in numbers):\n        raise TypeError(\"Input must be a list of numbers.\")\n    \n    n = len(numbers)\n    if n == 0:\n        return 0.0\n    \n    mean = sum(numbers) / n\n    mad = sum(abs(x - mean) for x in numbers) / n\n    return mad", "method": "tdd_humaneval", "model": "gpt-4o-mini", "timestamp": "2025-08-30T14:07:15.191847", "entry_point": "mean_absolute_deviation", "canonical_solution": "    mean = sum(numbers) / len(numbers)\n    return sum(abs(x - mean) for x in numbers) / len(numbers)\n", "test": "\n\nMETADATA = {\n    'author': 'jt',\n    'dataset': 'test'\n}\n\n\ndef check(candidate):\n    assert abs(candidate([1.0, 2.0, 3.0]) - 2.0/3.0) < 1e-6\n    assert abs(candidate([1.0, 2.0, 3.0, 4.0]) - 1.0) < 1e-6\n    assert abs(candidate([1.0, 2.0, 3.0, 4.0, 5.0]) - 6.0/5.0) < 1e-6\n\n", "generated_tests": "import pytest\n\ndef test_mean_absolute_deviation_basic_functionality():\n    assert mean_absolute_deviation([1.0, 2.0, 3.0, 4.0]) == 1.0\n    assert mean_absolute_deviation([1.0, 1.0, 1.0]) == 0.0\n    assert mean_absolute_deviation([1.0, 2.0, 3.0]) == 0.6666666666666666\n    assert mean_absolute_deviation([10.0, 20.0, 30.0]) == 10.0\n\ndef test_mean_absolute_deviation_with_negative_numbers():\n    assert mean_absolute_deviation([-1.0, 0.0, 1.0]) == 0.6666666666666666\n    assert mean_absolute_deviation([-10.0, -20.0, -30.0]) == 10.0\n\ndef test_mean_absolute_deviation_with_mixed_numbers():\n    assert mean_absolute_deviation([-1.0, 0.0, 1.0, 2.0]) == 1.0\n    assert mean_absolute_deviation([-5.0, 0.0, 5.0]) == 5.0\n\ndef test_mean_absolute_deviation_edge_cases():\n    assert mean_absolute_deviation([]) == 0.0\n    assert mean_absolute_deviation([0.0]) == 0.0\n    assert mean_absolute_deviation([1.0]) == 0.0\n\ndef test_mean_absolute_deviation_invalid_inputs():\n    with pytest.raises(TypeError):\n        mean_absolute_deviation(None)\n    with pytest.raises(TypeError):\n        mean_absolute_deviation(\"string\")\n    with pytest.raises(TypeError):\n        mean_absolute_deviation([1, 2, \"three\"])", "tdd_stage": "implementation"}
{"task_id": "HumanEval/5", "prompt": "from typing import List\n\n\ndef intersperse(numbers: List[int], delimeter: int) -> List[int]:\n    \"\"\" Insert a number 'delimeter' between every two consecutive elements of input list `numbers'\n    >>> intersperse([], 4)\n    []\n    >>> intersperse([1, 2, 3], 4)\n    [1, 4, 2, 4, 3]\n    \"\"\"\n", "completion": "if not isinstance(delimeter, int):\n        raise TypeError(\"delimeter must be an integer\")\n    if any(not isinstance(num, int) for num in numbers):\n        raise TypeError(\"all elements in numbers must be integers\")\n    \n    if not numbers:\n        return []\n    \n    result = []\n    for i in range(len(numbers)):\n        result.append(numbers[i])\n        if i < len(numbers) - 1:\n            result.append(delimeter)\n    \n    return result", "method": "tdd_humaneval", "model": "gpt-4o-mini", "timestamp": "2025-08-30T14:07:25.406068", "entry_point": "intersperse", "canonical_solution": "    if not numbers:\n        return []\n\n    result = []\n\n    for n in numbers[:-1]:\n        result.append(n)\n        result.append(delimeter)\n\n    result.append(numbers[-1])\n\n    return result\n", "test": "\n\nMETADATA = {\n    'author': 'jt',\n    'dataset': 'test'\n}\n\n\ndef check(candidate):\n    assert candidate([], 7) == []\n    assert candidate([5, 6, 3, 2], 8) == [5, 8, 6, 8, 3, 8, 2]\n    assert candidate([2, 2, 2], 2) == [2, 2, 2, 2, 2]\n", "generated_tests": "import pytest\n\ndef test_intersperse_with_empty_list():\n    assert intersperse([], 4) == []\n\ndef test_intersperse_with_single_element():\n    assert intersperse([1], 4) == [1]\n\ndef test_intersperse_with_two_elements():\n    assert intersperse([1, 2], 4) == [1, 4, 2]\n\ndef test_intersperse_with_multiple_elements():\n    assert intersperse([1, 2, 3], 4) == [1, 4, 2, 4, 3]\n\ndef test_intersperse_with_negative_numbers():\n    assert intersperse([-1, -2, -3], 0) == [-1, 0, -2, 0, -3]\n\ndef test_intersperse_with_zero_delimeter():\n    assert intersperse([1, 2, 3], 0) == [1, 0, 2, 0, 3]\n\ndef test_intersperse_with_large_numbers():\n    assert intersperse([1000, 2000, 3000], 500) == [1000, 500, 2000, 500, 3000]\n\ndef test_intersperse_with_identical_elements():\n    assert intersperse([5, 5, 5], 2) == [5, 2, 5, 2, 5]\n\ndef test_intersperse_with_delimeter_as_negative():\n    assert intersperse([1, 2, 3], -1) == [1, -1, 2, -1, 3]\n\ndef test_intersperse_with_large_list():\n    assert intersperse(list(range(100)), 99) == [i for i in range(100) for _ in (0, 1)][:-1] + [99]\n\ndef test_intersperse_with_non_integer_delimeter():\n    with pytest.raises(TypeError):\n        intersperse([1, 2, 3], 'a')\n\ndef test_intersperse_with_non_integer_elements():\n    with pytest.raises(TypeError):\n        intersperse([1, 2, '3'], 4)", "tdd_stage": "implementation"}
{"task_id": "HumanEval/6", "prompt": "from typing import List\n\n\ndef parse_nested_parens(paren_string: str) -> List[int]:\n    \"\"\" Input to this function is a string represented multiple groups for nested parentheses separated by spaces.\n    For each of the group, output the deepest level of nesting of parentheses.\n    E.g. (()()) has maximum two levels of nesting while ((())) has three.\n\n    >>> parse_nested_parens('(()()) ((())) () ((())()())')\n    [2, 3, 1, 3]\n    \"\"\"\n", "completion": "if not paren_string.strip():\n        return []\n    \n    groups = paren_string.split()\n    depths = []\n    \n    for group in groups:\n        max_depth = 0\n        current_depth = 0\n        \n        for char in group:\n            if char == '(':\n                current_depth += 1\n                max_depth = max(max_depth, current_depth)\n            elif char == ')':\n                current_depth -= 1\n        \n        depths.append(max_depth)\n    \n    return depths", "method": "tdd_humaneval", "model": "gpt-4o-mini", "timestamp": "2025-08-30T14:07:38.796591", "entry_point": "parse_nested_parens", "canonical_solution": "    def parse_paren_group(s):\n        depth = 0\n        max_depth = 0\n        for c in s:\n            if c == '(':\n                depth += 1\n                max_depth = max(depth, max_depth)\n            else:\n                depth -= 1\n\n        return max_depth\n\n    return [parse_paren_group(x) for x in paren_string.split(' ') if x]\n", "test": "\n\nMETADATA = {\n    'author': 'jt',\n    'dataset': 'test'\n}\n\n\ndef check(candidate):\n    assert candidate('(()()) ((())) () ((())()())') == [2, 3, 1, 3]\n    assert candidate('() (()) ((())) (((())))') == [1, 2, 3, 4]\n    assert candidate('(()(())((())))') == [4]\n", "generated_tests": "import pytest\n\ndef test_basic_functionality():\n    assert parse_nested_parens('(()()) ((())) () ((())()())') == [2, 3, 1, 3]\n    assert parse_nested_parens('() () ()') == [1, 1, 1]\n    assert parse_nested_parens('((()))') == [3]\n    assert parse_nested_parens('()()()') == [1]\n    assert parse_nested_parens('((())())') == [2]\n\ndef test_edge_cases():\n    assert parse_nested_parens('') == []\n    assert parse_nested_parens(' ') == []\n    assert parse_nested_parens('() () () ()') == [1, 1, 1, 1]\n    assert parse_nested_parens('((((()))))') == [5]\n    assert parse_nested_parens('()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()", "tdd_stage": "implementation"}
{"task_id": "HumanEval/7", "prompt": "from typing import List\n\n\ndef filter_by_substring(strings: List[str], substring: str) -> List[str]:\n    \"\"\" Filter an input list of strings only for ones that contain given substring\n    >>> filter_by_substring([], 'a')\n    []\n    >>> filter_by_substring(['abc', 'bacd', 'cde', 'array'], 'a')\n    ['abc', 'bacd', 'array']\n    \"\"\"\n", "completion": "if not all(isinstance(s, str) for s in strings):\n        raise TypeError(\"All elements in the input list must be strings.\")\n    return [s for s in strings if substring in s]", "method": "tdd_humaneval", "model": "gpt-4o-mini", "timestamp": "2025-08-30T14:07:47.243529", "entry_point": "filter_by_substring", "canonical_solution": "    return [x for x in strings if substring in x]\n", "test": "\n\nMETADATA = {\n    'author': 'jt',\n    'dataset': 'test'\n}\n\n\ndef check(candidate):\n    assert candidate([], 'john') == []\n    assert candidate(['xxx', 'asd', 'xxy', 'john doe', 'xxxAAA', 'xxx'], 'xxx') == ['xxx', 'xxxAAA', 'xxx']\n    assert candidate(['xxx', 'asd', 'aaaxxy', 'john doe', 'xxxAAA', 'xxx'], 'xx') == ['xxx', 'aaaxxy', 'xxxAAA', 'xxx']\n    assert candidate(['grunt', 'trumpet', 'prune', 'gruesome'], 'run') == ['grunt', 'prune']\n", "generated_tests": "import pytest\n\ndef test_empty_list():\n    assert filter_by_substring([], 'a') == []\n\ndef test_no_matching_substring():\n    assert filter_by_substring(['abc', 'def', 'ghi'], 'z') == []\n\ndef test_single_matching_substring():\n    assert filter_by_substring(['abc', 'def', 'ghi'], 'a') == ['abc']\n\ndef test_multiple_matching_substrings():\n    assert filter_by_substring(['abc', 'bacd', 'cde', 'array'], 'a') == ['abc', 'bacd', 'array']\n\ndef test_substring_at_start():\n    assert filter_by_substring(['apple', 'banana', 'cherry'], 'ap') == ['apple']\n\ndef test_substring_at_end():\n    assert filter_by_substring(['apple', 'banana', 'cherry'], 'ry') == ['cherry']\n\ndef test_substring_in_middle():\n    assert filter_by_substring(['apple', 'banana', 'cherry'], 'nan') == ['banana']\n\ndef test_case_sensitivity():\n    assert filter_by_substring(['Apple', 'banana', 'Cherry'], 'a') == ['banana']\n\ndef test_substring_is_empty():\n    assert filter_by_substring(['abc', 'def', 'ghi'], '') == ['abc', 'def', 'ghi']\n\ndef test_substring_is_whitespace():\n    assert filter_by_substring(['abc', ' def ', 'ghi'], ' ') == [' def ']\n\ndef test_special_characters():\n    assert filter_by_substring(['abc!', 'def@', '#ghi'], '#') == ['#ghi']\n\ndef test_numeric_strings():\n    assert filter_by_substring(['123', '456', '789'], '2') == ['123']\n\ndef test_mixed_content():\n    assert filter_by_substring(['abc', '123', 'def', '456'], '1') == ['123']\n\ndef test_long_strings():\n    long_strings = ['a' * 1000, 'b' * 1000, 'c' * 1000]\n    assert filter_by_substring(long_strings, 'a') == ['a' * 1000]\n\ndef test_non_string_elements():\n    with pytest.raises(TypeError):\n        filter_by_substring(['abc', 123, 'def'], 'a')", "tdd_stage": "implementation"}
{"task_id": "HumanEval/8", "prompt": "from typing import List, Tuple\n\n\ndef sum_product(numbers: List[int]) -> Tuple[int, int]:\n    \"\"\" For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list.\n    Empty sum should be equal to 0 and empty product should be equal to 1.\n    >>> sum_product([])\n    (0, 1)\n    >>> sum_product([1, 2, 3, 4])\n    (10, 24)\n    \"\"\"\n", "completion": "if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list.\")\n    \n    total_sum = 0\n    total_product = 1\n    for number in numbers:\n        if not isinstance(number, int):\n            raise TypeError(\"All elements in the list must be integers.\")\n        total_sum += number\n        total_product *= number\n    \n    return total_sum, total_product", "method": "tdd_humaneval", "model": "gpt-4o-mini", "timestamp": "2025-08-30T14:07:54.333482", "entry_point": "sum_product", "canonical_solution": "    sum_value = 0\n    prod_value = 1\n\n    for n in numbers:\n        sum_value += n\n        prod_value *= n\n    return sum_value, prod_value\n", "test": "\n\nMETADATA = {\n    'author': 'jt',\n    'dataset': 'test'\n}\n\n\ndef check(candidate):\n    assert candidate([]) == (0, 1)\n    assert candidate([1, 1, 1]) == (3, 1)\n    assert candidate([100, 0]) == (100, 0)\n    assert candidate([3, 5, 7]) == (3 + 5 + 7, 3 * 5 * 7)\n    assert candidate([10]) == (10, 10)\n", "generated_tests": "import pytest\n\ndef test_sum_product_empty_list():\n    assert sum_product([]) == (0, 1)\n\ndef test_sum_product_single_element():\n    assert sum_product([5]) == (5, 5)\n    assert sum_product([-3]) == (-3, -3)\n\ndef test_sum_product_multiple_positive_elements():\n    assert sum_product([1, 2, 3, 4]) == (10, 24)\n    assert sum_product([10, 20, 30]) == (60, 6000)\n\ndef test_sum_product_multiple_negative_elements():\n    assert sum_product([-1, -2, -3]) == (-6, -6)\n    assert sum_product([-1, -2, 3]) == (0, 6)\n\ndef test_sum_product_mixed_elements():\n    assert sum_product([-1, 0, 1]) == (0, 0)\n    assert sum_product([-1, 2, 3]) == (4, -6)\n\ndef test_sum_product_large_numbers():\n    assert sum_product([1000, 2000, 3000]) == (6000, 6000000000)\n\ndef test_sum_product_with_zeros():\n    assert sum_product([0, 0, 0]) == (0, 0)\n    assert sum_product([0, 1, 2]) == (3, 0)\n\ndef test_sum_product_invalid_input():\n    with pytest.raises(TypeError):\n        sum_product(None)\n    with pytest.raises(TypeError):\n        sum_product([1, 'two', 3])\n    with pytest.raises(TypeError):\n        sum_product([1, 2.5, 3])", "tdd_stage": "implementation"}
{"task_id": "HumanEval/9", "prompt": "from typing import List, Tuple\n\n\ndef rolling_max(numbers: List[int]) -> List[int]:\n    \"\"\" From a given list of integers, generate a list of rolling maximum element found until given moment\n    in the sequence.\n    >>> rolling_max([1, 2, 3, 2, 3, 4, 2])\n    [1, 2, 3, 3, 3, 4, 4]\n    \"\"\"\n", "completion": "if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list.\")\n    if any(not isinstance(x, int) for x in numbers):\n        raise TypeError(\"All elements in the list must be integers.\")\n    \n    rolling_max_list = []\n    current_max = float('-inf')\n    \n    for number in numbers:\n        current_max = max(current_max, number)\n        rolling_max_list.append(current_max)\n    \n    return rolling_max_list", "method": "tdd_humaneval", "model": "gpt-4o-mini", "timestamp": "2025-08-30T14:08:05.318401", "entry_point": "rolling_max", "canonical_solution": "    running_max = None\n    result = []\n\n    for n in numbers:\n        if running_max is None:\n            running_max = n\n        else:\n            running_max = max(running_max, n)\n\n        result.append(running_max)\n\n    return result\n", "test": "\n\nMETADATA = {\n    'author': 'jt',\n    'dataset': 'test'\n}\n\n\ndef check(candidate):\n    assert candidate([]) == []\n    assert candidate([1, 2, 3, 4]) == [1, 2, 3, 4]\n    assert candidate([4, 3, 2, 1]) == [4, 4, 4, 4]\n    assert candidate([3, 2, 3, 100, 3]) == [3, 3, 3, 100, 100]\n", "generated_tests": "import pytest\n\ndef test_basic_functionality():\n    assert rolling_max([1, 2, 3, 2, 3, 4, 2]) == [1, 2, 3, 3, 3, 4, 4]\n    assert rolling_max([5, 3, 6, 2, 8]) == [5, 5, 6, 6, 8]\n    assert rolling_max([-1, -2, -3, -4]) == [-1, -1, -1, -1]\n    assert rolling_max([0, 0, 0, 0]) == [0, 0, 0, 0]\n\ndef test_edge_cases():\n    assert rolling_max([]) == []\n    assert rolling_max([10]) == [10]\n    assert rolling_max([1, 1, 1, 1, 1]) == [1, 1, 1, 1, 1]\n    assert rolling_max([1, 2, 1, 2, 1]) == [1, 2, 2, 2, 2]\n\ndef test_negative_numbers():\n    assert rolling_max([-5, -1, -3, -2]) == [-5, -1, -1, -1]\n    assert rolling_max([-10, -20, -30, -5]) == [-10, -10, -10, -5]\n\ndef test_mixed_numbers():\n    assert rolling_max([-1, 0, 1, -1, 2]) == [-1, 0, 1, 1, 2]\n    assert rolling_max([3, -1, 4, 0, 2]) == [3, 3, 4, 4, 4]\n\ndef test_large_numbers():\n    assert rolling_max([1000000, 999999, 1000001]) == [1000000, 1000000, 1000001]\n    assert rolling_max([1, 2, 3, 1000000, 999999]) == [1, 2, 3, 1000000, 1000000]\n\ndef test_invalid_inputs():\n    with pytest.raises(TypeError):\n        rolling_max(None)\n    with pytest.raises(TypeError):\n        rolling_max(\"string\")\n    with pytest.raises(TypeError):\n        rolling_max([1, 2, \"three\"])", "tdd_stage": "implementation"}
